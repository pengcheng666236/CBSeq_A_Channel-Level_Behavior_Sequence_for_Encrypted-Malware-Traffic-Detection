import json
import os
import random
import statistics
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import preprocess_dbscan
import tools

SAMPLE_NUM = 10000
max_sequence_length = 16
device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')
# 定义模型参数
EMBED_SIZE = 96
HIDDEN_SIZE = 256
NUM_HEADS = 8
FEED_FORWARD_SIZE = 96
NUM_BLOCKS = 6
NUM_CLASSES = 109 + 31 + 1
BATCH_SIZE = 8
# 要多一个类,n->n+1，用来表示噪声类或者未知类
# NUM_CLASSES = 32
num_epochs = 20

# 写之前在捋一下输入结构。神经网络的dataloader能根据输入的x，取出需要的特征字段

"""
模型的子层，包括多头注意力和前馈网络，以及 Add & Norm 模块：
class MultiHeadAttention(nn.Module):
    def __init__(self, embed_size, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.embed_size = embed_size
        self.num_heads = num_heads

        self.head_dim = self.embed_size // num_heads
        # self.head_dim = 1  # 先用单头测试
        
        assert self.head_dim * num_heads == self.hidden_size, "Embedding size must be divisible by number of heads"
        # kqv都是表示注意力的层，不会影响维度，具体是通过kqv对应的权重矩阵来得到kqv层的输入
        self.query = nn.Linear(embed_size,embed_size)
        self.key = nn.Linear(embed_size,embed_size)
        self.value = nn.Linear(embed_size,embed_size)

        self.fc_out = nn.Linear(embed_size, embed_size)
    
    def forward(self, query, key, value, mask=None):
        # 获取 batch size
        batch_size = query.shape[0]
        
        # 线性变换,EMBED_SIZE * NUM_HEADS的输入，乘以NUM_HEADS * EMBED_SIZE的权重，得到EMBED_SIZE * EMBED_SIZE的Q，K，V
        # 假设输入的query是NUM_HEADS * EMBED_SIZE，
        #     nn.Linear(embed_size,embed_size)就表示接受列数是EMBED_SIZE的任何输入query，得到EMBED_SIZE * EMBED_SIZE的输出Q
        # 实际计算得到的结果和网上文章讲的是一样的
        Q = self.query(query.T)
        K = self.key(key.T)
        V = self.value(value.T)
        
        # 拆分成多头
        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
        K = K.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
        V = V.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
        
        # 用Q和K计算注意力
        # torch.einsum进行批次矩阵乘法操作，计算 Q 和 K 的点积。然后将结果除以 self.head_dim 的平方根以缩放点积，避免点积过大导致梯度爆炸。
        energy = torch.einsum("nqhd, nkhd -> nhqk", [Q, K]) / (self.head_dim ** 0.5)
        # 如果存在掩码（mask），则将掩码中为零的位置对应的能量设置为一个非常小的值，这样在 softmax 操作时会接近于零，达到屏蔽某些位置的效果。
        if mask is not None:
            energy = energy.masked_fill(mask == 0, float("-1e20"))
        # 对能量张量进行 softmax 操作，得到注意力权重。
        attention = torch.softmax(energy, dim=-1)
        
        # 使用注意力权重对值张量 V 进行加权求和，然后通过批次矩阵乘法计算注意力输出。
        x = torch.einsum("nhqk, nkhd -> nqhd", [attention, V]).reshape(batch_size, -1, self.embed_size // self.num_heads)
        
        # 输出并线性变换
        x = self.fc_out(x)
        return x
"""


# nn.TransformerEncoderLayer 期望输入的形状是 (seq_length, batch_size, embed_size)
class MultiHeadAttention(nn.Module):
    def __init__(self, embed_size, num_heads):
        super(MultiHeadAttention, self).__init__()
        # self.attention = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads).to(device)
        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=num_heads).to(device)
        self.linear = nn.Linear(embed_size, embed_size).to(device)
        self.norm = nn.LayerNorm(embed_size).to(device)

    def forward(self, x):
        # 在 Transformer 中，输入x应该是 (seq_length, batch_size, embed_size) 的形状
        x = x.to(device)
        # print(x.shape) # torch.Size([1, 8, 96]) # torch.Size([8, 16, 100])
        x = x.permute(1, 0, 2)
        # attn_output = self.attention(x)
        attn_output, attn_output_weights = self.attention(x,x,x)
        x = x + attn_output
        x = self.norm(x)
        output = self.linear(x)
        # 返回的形状为 (batch_size, seq_length, embed_size)
        return attn_output.permute(1, 0, 2)

# 前向传播网络层
class FeedForward(nn.Module):
    def __init__(self, embed_size, feed_forward_size):
        super(FeedForward, self).__init__()
        self.linear_1 = nn.Linear(embed_size, feed_forward_size)
        self.linear_2 = nn.Linear(feed_forward_size, embed_size)
    
    def forward(self, x):
        x = F.relu(self.linear_1(x))
        x = self.linear_2(x)
        return x

# 正则化
class AddNorm(nn.Module):
    def __init__(self, embed_size):
        super(AddNorm, self).__init__()
        self.norm = nn.LayerNorm(embed_size).to(device)
    
    def forward(self, x, sublayer):
        return x + sublayer(self.norm(x))


# 前馈网络
class FFN(nn.Module):
    #                  embed_size, HIDDEN_SIZE,feed_forward_size
    def __init__(self, input_size, hidden_size, output_size):
        super(FFN, self).__init__()
        # self.W1 = nn.Parameter(torch.randn(input_size, hidden_size)).to(device)
        # self.b1 = nn.Parameter(torch.randn(hidden_size)).to(device)
        # self.W2 = nn.Parameter(torch.randn(hidden_size, output_size)).to(device)
        # self.b2 = nn.Parameter(torch.randn(output_size)).to(device)
        self.W1 = nn.Parameter(torch.empty(input_size, hidden_size, device=device))
        self.b1 = nn.Parameter(torch.empty(hidden_size, device=device))
        self.W2 = nn.Parameter(torch.empty(hidden_size, output_size, device=device))
        self.b2 = nn.Parameter(torch.empty(output_size, device=device))
        self.reset_parameters()
    
    def reset_parameters(self):
        nn.init.xavier_uniform_(self.W1)
        nn.init.zeros_(self.b1)
        nn.init.xavier_uniform_(self.W2)
        nn.init.zeros_(self.b2)

    def forward(self, x):
        # print(x.shape) # (8,16,96)
        # 第一层的线性变换和 ReLU 激活函数
        x = x.reshape(-1, EMBED_SIZE)
        hidden = F.relu(torch.mm(x, self.W1) + self.b1)
        # 第二层的线性变换
        output = torch.mm(hidden, self.W2) + self.b2

        output = output.reshape(BATCH_SIZE, max_sequence_length, -1)
        return output

"""
    和下面这个逻辑等价
    class FFN(nn.Module):
        def __init__(self, input_size, hidden_size):
            super(FFN, self).__init__()
            self.fc1 = nn.Linear(input_size, hidden_size)
            self.fc2 = nn.Linear(hidden_size, input_size)
        
        def forward(self, x):
            x = F.relu(self.fc1(x))
            x = self.fc2(x)
            return x
"""

# 定义一个 Encoder Block，它包含一个多头注意力子层和一个前馈网络子层，以及 Add & Norm 模块
class EncoderBlock(nn.Module):
    def __init__(self, embed_size, num_heads, feed_forward_size):
        super(EncoderBlock, self).__init__()
        self.multihead_attention = MultiHeadAttention(embed_size, num_heads)
        
        self.feed_forward = FFN(embed_size, HIDDEN_SIZE,feed_forward_size)
        self.add_norm_1 = AddNorm(embed_size)
        self.add_norm_2 = AddNorm(embed_size)
    
    def forward(self, x):
        # 第一个子层：多头注意力,x的维度必须是(embed_size, num_heads)
        # attn_output = self.multihead_attention(x, x, x)
        # attn_output = self.multihead_attention(x.unsqueeze(0))
        attn_output = self.multihead_attention(x)
        x = self.add_norm_1(attn_output,self.multihead_attention)
        # print(x.shape) # torch.Size([1, 8, 96])
        x = x.squeeze()
        # print(x.shape) # torch.Size([8, 96])

        # 第二个子层：前馈网络
        ff_output = self.feed_forward(x)
        x = self.add_norm_2(ff_output, self.feed_forward)
        
        return x

# 每个Encoder包含六个block,需要创建四个并行的实例分别处理四个序列
class Encoder(nn.Module):
    def __init__(self, embed_size, num_heads, feed_forward_size,num_blocks = 6):
        super(Encoder, self).__init__()
        self.blocks = nn.ModuleList([
            EncoderBlock(embed_size, num_heads, feed_forward_size)
            for _ in range(num_blocks)
        ])
    
    def forward(self, x):
        for block in self.blocks:
            x = block(x)
        return x


# 分类器，依次输入8个样本，输出8个样本对应的预测分类
class Classifier(nn.Module):
    def __init__(self, input_size, num_classes):
        super(Classifier, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(1).to(device)
        self.linear = nn.Linear(input_size, num_classes).to(device)
    
    def forward(self, x):
        # 平均池化
        # print("classifier:",x.shape) # torch.Size([8, 384]) ，8个样本，每个样本的4个长度为96的特征序列拼接成最终的特征表示
        # x = x.view(x.size(0), -1)
        # print("classifier viewed:",x.shape)
        # x = self.avg_pool(x.unsqueeze(-1))
        # # print(x.shape) # torch.Size([8, 384, 1])
        # x = x.squeeze(-1)
        # # print(x.shape) # torch.Size([8, 384])

        # # 线性层
        # x = self.linear(x)
        # print(x.shape) # torch.Size([8, 140])
        # # 一个batch的所有flow样本的预测结果，一共多少各类即有多少列
        # x = x.reshape(BATCH_SIZE, NUM_CLASSES)
        x = x.permute(0, 2, 1)  # 变成 [batch_size, embed_size, seq_length]
        x = self.avg_pool(x).squeeze(-1)  # 变成 [batch_size, embed_size]
        x = self.linear(x)  # 变成 [batch_size, num_classes]
        return x




# 定义 MSFormer 模型，它由四个相同结构的 Encoder 组成，分别处理一个特征序列，每个特征序列都是behavior sequence的 word embedding表示
class MSFormer(nn.Module):
    def __init__(self, embed_size, num_heads, feed_forward_size, num_blocks, num_classes):
        super(MSFormer, self).__init__()
        self.pn_encoder = Encoder(embed_size, num_heads, feed_forward_size, num_blocks)
        self.iat_encoder = Encoder(embed_size, num_heads, feed_forward_size, num_blocks)
        self.sp_encoder = Encoder(embed_size, num_heads, feed_forward_size, num_blocks)
        self.dp_encoder = Encoder(embed_size, num_heads, feed_forward_size, num_blocks)
    
        self.classifier = Classifier(embed_size * 4, num_classes)  # 乘以 4 是因为有四种类型的序列

    def forward(self, pn_sequences, iat_sequences, sp_sequences, dp_sequences):

        pn_encoded = self.pn_encoder(pn_sequences)
        iat_encoded = self.iat_encoder(iat_sequences)
        sp_encoded = self.sp_encoder(sp_sequences)
        dp_encoded = self.dp_encoder(dp_sequences)

        # 将四个序列的编码结果拼接起来
        encoded = torch.cat([pn_encoded, iat_encoded, sp_encoded, dp_encoded], dim=-1)
        # print("encoded shape:",encoded.shape) # encoded shape: torch.Size([8, 384]),还是原始的8个样本，四个特征各有96维度，并起来384
        
        # 输入到分类器
        output = self.classifier(encoded)
        # print("out shape:",output.shape) # out shape: torch.Size([140])
        return output


# # 输入：behavior_sequence,生成一个可迭代样本的Dataset
# class Behavior_Embeded_sequence_Dataset(Dataset):
#     def __init__(self, behavior_sequences):
#         self.samples = []
#         """
#         label要改成对应的file_label
#         所以预聚类的目的就是，确保预聚类内部的点都是同一个类的，真正分类的标签还得是文件标签，而不是预聚类的
#         仅仅修改了这一处，几乎是尘埃落定，可以开始测试
#         """
#         for i, sequences in behavior_sequences.items():
#             # cluster的第i个channel的值对应的嵌入词向量
#             pn_cluster = sequences['embedded_pn_cluster']
#             # print("len(pn_cluster):",len(pn_cluster))
#             iat_cluster = sequences['embedded_iat_cluster']
#             sp_cluster = sequences['embedded_sp_cluster']
#             dp_cluster = sequences['embedded_dp_cluster']
#             label = sequences["file_label"]
#             # 每个通道的特征长度相同
#             num_channels = len(pn_cluster)
#             for i in range(num_channels):
#                 # 获取当前通道的特征值
#                 # print("pn_features:",pn_cluster[i])
#                 # print("labe:",label)
#                 # pn_features: [ -0.01930413767695427, 0.02052118442952633],不止这么长，但是一个一维list
#                 # 也就是这个channel所有flow的特征合并的list，一个位置对应一个flow的特征值

#                 pn_features = torch.tensor(pn_cluster[i])
#                 # if len(pn_features) != 100:
#                 #     print(len(pn_features)) # 全是100=embedsize,没有问题
#                 iat_features = torch.tensor(iat_cluster[i])
#                 sp_features = torch.tensor(sp_cluster[i])
#                 dp_features = torch.tensor(dp_cluster[i])
                
#                 # 构建样本，包括特征和标签
#                 sample = {
#                     'pn_features': pn_features,
#                     'iat_features': iat_features,
#                     'sp_features': sp_features,
#                     'dp_features': dp_features,
#                     'label': label
#                 }
#                 self.samples.append(sample)
#             # print("sample_len:",len(self.samples))
    
#     def __len__(self):
#         return len(self.samples)
    
#     def __getitem__(self, idx):
#         return self.samples[idx]
# 输入：behavior_sequence,生成一个可迭代样本的Dataset
class Behavior_Embeded_sequence_Dataset(Dataset):
    def __init__(self, behavior_sequences):
        self.samples = []
        """
        label要改成对应的file_label
        所以预聚类的目的就是，确保预聚类内部的点都是同一个类的，真正分类的标签还得是文件标签，而不是预聚类的
        仅仅修改了这一处，几乎是尘埃落定，可以开始测试
        """
        for i, sequences in behavior_sequences.items():
            # cluster的第i个channel的值对应的嵌入词向量
            pn_cluster = sequences['embedded_pn_cluster']
            if len(pn_cluster) < max_sequence_length:
                pn_cluster += [[0]*100] * (max_sequence_length - len(pn_cluster))
            elif len(pn_cluster) > max_sequence_length:
                pn_cluster = pn_cluster[:max_sequence_length]
            iat_cluster = sequences['embedded_iat_cluster']
            if len(iat_cluster) < max_sequence_length:
                iat_cluster += [[0.0]*100] * (max_sequence_length - len(iat_cluster))
            elif len(iat_cluster) > max_sequence_length:
                iat_cluster = iat_cluster[:max_sequence_length]
            sp_cluster = sequences['embedded_sp_cluster']
            if len(sp_cluster) < max_sequence_length:
                sp_cluster += [[0]*100] * (max_sequence_length - len(sp_cluster))
            elif len(sp_cluster) > max_sequence_length:
                sp_cluster = sp_cluster[:max_sequence_length]
            dp_cluster = sequences['embedded_dp_cluster']
            if len(dp_cluster) < max_sequence_length:
                dp_cluster += [[0]*100] * (max_sequence_length - len(dp_cluster))
            elif len(dp_cluster) > max_sequence_length:
                dp_cluster = dp_cluster[:max_sequence_length]
            label = sequences["file_label"]
            # 构建样本，包括特征和标签
            sample = {
                'pn_features': np.array(pn_cluster,dtype=np.float32),
                'iat_features': np.array(iat_cluster,dtype=np.float32),
                'sp_features': np.array(sp_cluster,dtype=np.float32),
                'dp_features': np.array(dp_cluster,dtype=np.float32),
                'label': label
            }
            self.samples.append(sample)
            # print("sample_len:",len(self.samples))
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        return self.samples[idx]


# 如果要写成loader，数据就不能再是预聚类的形式，必须是一个个单独channel的形式，预聚类的目的就是保留了预聚类对应的label
def trial(behavior_sequences):
    # 1.划分数据
    keys = list(behavior_sequences.keys()) # 将字典的键转换为列表

    random.seed(123)  
    random.shuffle(keys)  # 使用随机采样或确定性划分方法划分训练集和测试集的键列表
    train_size = int(0.8 * len(keys))
    train_keys = keys[:train_size]
    test_keys = keys[train_size:]
    
    behavior_sequences_train = {key: behavior_sequences[key] for key in train_keys} # 训练集和测试集的字典
    behavior_sequences_test = {key: behavior_sequences[key] for key in test_keys}

    # 2.创建自定义数据集对象
    train_dataset = Behavior_Embeded_sequence_Dataset(behavior_sequences=behavior_sequences_train)  # your_data_dict 是包含预聚类数据的字典
    test_dataset = Behavior_Embeded_sequence_Dataset(behavior_sequences=behavior_sequences_test)  # your_data_dict 是包含预聚类数据的字典
    batch_size = 8  # 设置批大小
    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)  
    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True,drop_last=True)  
    
    # 3.训练
    # 定义模型、损失函数和优化器
    model = MSFormer(embed_size=EMBED_SIZE, num_heads=NUM_HEADS, feed_forward_size=FEED_FORWARD_SIZE, num_blocks=NUM_BLOCKS, num_classes=NUM_CLASSES)
    criterion = nn.CrossEntropyLoss()
    """
        nn.CrossEntropyLoss(input,target)
        input: 这是模型的输出，形状为 (N, C)，其中 N 是批量大小（batch size），C 是类别的数量。
        这些值通常是未经过 softmax 的原始 logits。
        target: 这是真实的类别标签，形状为 (N,)，包含每个样本的正确类别的索引。
        每个值是一个整数，表示类别索引(范围是 [0, C-1])。
    """
    optimizer = optim.Adam(model.parameters(), lr=0.00001)
    # 获取模型输出的第i类对应的label是哪个字符串
    with open('/home/lxc/CBSeq/data/result/relations_dbscan.json', 'r') as json_file:
        relations = json.load(json_file)
    with open('/home/lxc/CBSeq/data/result/relations_reverse_dbscan.json', 'r') as json_file:
        relations_reverse = json.load(json_file)


    if not os.path.exists("/home/lxc/CBSeq/models/MSFormer_dbscan_{}.pth".format(str(SAMPLE_NUM))):
        for epoch in range(num_epochs):
            print("Epoch:",epoch)
            model.train()  # 设置模型为训练模式
            running_loss = 0.0
            # 使用 DataLoader 加载数据.一次包含8个样本和8个标签
            # 在最后一个dataloader的batch，会出现样本数不足以构成一个batch的情况，要在数据集上设置drop_last=True
            for i, data in enumerate(train_loader, 0):  
                # 获取8个样本的特征
                pn_features = data['pn_features'][:,:,:96].to(device)
                # print(pn_features.shape)
                # if pn_features.shape[0] == 1:
                #     pn_features = pn_features.unsqueeze(0)
                # print(pn_features.shape) # torch.Size([8, 96])，除了最后一个batch样本不足
                # embed_size=100,所以每一个样本的特征sequence的数值，都映射到100维的词向量
                iat_features = data['iat_features'][:,:,:96].to(device)
                sp_features = data['sp_features'][:,:,:96].to(device)
                dp_features = data['dp_features'][:,:,:96].to(device)
                labels = data['label']
                # print(len(labels)) # 8
                
                # 梯度清零
                optimizer.zero_grad()
                # 前向传播
                outputs = model(pn_features, iat_features, sp_features, dp_features).cpu()

                # 计算损失
                
                # 把output转换成可理解的输出
                # print("outputs:",outputs.shape) # torch.Size([8,141])
                # probabilities = torch.softmax(outputs, dim=1)
                # # print("outputs softmax:",probabilities.shape,probabilities[0]) # torch.Size([8,140])
                # _, predicted = torch.max(probabilities, 1)
                # # print(predicted) # tensor([ 88, 102, 104, 102, 102, 102, 102,  88])
                # predicted_classes = tools.map_tensor_to_string(predicted.cpu().numpy(), relations)
                # print(predicted_classes) 
                # # ['scp1', 'scpDown5', 'scpDown5', 'scpDown5', 'scp1', 'scpDown5', 'scpDown5', 'vpn_skype_files1b']

                # print("shapes of labels:",len(labels),labels) # labels 是一个形状为 [8, 1] 的PyTorch张量
                # # ['gmailchat2', 'facebook_video2b', 'skype_file3', 'voipbuster_4a', 'voipbuster1b', 'youtube5', 'vpn_ftps_A', 'vpn_skype_audio1']
                # 但是计算交叉熵的输入是数字形式，因此需要的是反向索引而不是正向索引
                labels_index = tools.map_tensor_to_string(labels, relations_reverse)
                labels_index = torch.tensor(labels_index)
                # print(labels_index) # [87, 99, 111, 115, 136, 67, 83, 58]
                loss = criterion(outputs, labels_index)

                # 反向传播和优化
                loss.backward()
                optimizer.step()
                # 统计损失
                running_loss += loss.item()
                if i % 100 == 99:  # 每 100 个 mini-batches 打印一次训练状态
                    print(f'Epoch {epoch+1}, Batch {i+1}, Loss: {running_loss / 100}')
                    running_loss = 0.0
        torch.save(model.state_dict(),'/home/lxc/CBSeq/models/MSFormer_dbscan_{}.pth'.format(str(SAMPLE_NUM)))
    # 测试
    model.load_state_dict(torch.load("/home/lxc/CBSeq/models/MSFormer_dbscan_{}.pth".format(str(SAMPLE_NUM)))) 
    model.eval()  # 设置模型为评估模式

    total_correct = 0
    total_samples = 0
    total_loss = 0.0

    with torch.no_grad():
        all_preds = []
        all_labels = []
        for data in test_loader:  # 假设使用 DataLoader 加载测试数据
            pn_features = data['pn_features'][:,:,:96]
            iat_features = data['iat_features'][:,:,:96]
            sp_features = data['sp_features'][:,:,:96]
            dp_features = data['dp_features'][:,:,:96]
            labels = data['label']

            labels_index = tools.map_tensor_to_string(labels, relations_reverse)
            labels_index = torch.tensor(labels_index)

            outputs = model(pn_features, iat_features, sp_features, dp_features).cpu()
            # print("outputs:",outputs.shape,outputs)

            loss = criterion(outputs,labels_index)
            total_loss += loss.item()

            # 获取预测结果
            probabilities = torch.softmax(outputs, dim=1)
            _, predicted = torch.max(probabilities, 1)

            # print(predicted) # tensor([ 88, 102, 104, 102, 102, 102, 102,88])
            predicted_classes = tools.map_tensor_to_string(predicted.cpu().numpy(), relations)
            # print("predicted_classes:",predicted_classes) 
            # ['scp1', 'scpDown5', 'scpDown5', 'scpDown5', 'scp1', 'scpDown5', 'scpDown5', 'vpn_skype_files1b']
            # print("shapes of labels:",len(labels),labels) # labels 是一个形状为 [8, 1] 的PyTorch张量
            # ['gmailchat2', 'facebook_video2b', 'skype_file3', 'voipbuster_4a', 'voipbuster1b', 'youtube5', 'vpn_ftps_A', 'vpn_skype_audio1']
            
            _, predicted = torch.max(outputs, 0)
            # print("predicted_classes:",predicted_classes)
            # print("labels:",labels)
            total_correct += sum(1 for x, y in zip(predicted_classes, labels) if x == y)
            total_samples += len(labels)
            # 收集所有的真实标签和预测标签
            all_preds.append(predicted_classes)
            all_labels.append(labels)

        # 计算混淆矩阵
        all_preds = np.concatenate(all_preds)
        all_labels = np.concatenate(all_labels)
        conf_matrix = confusion_matrix(all_labels, all_preds)
        np.savetxt("/home/lxc/CBSeq/data/result/confusion_matrix_dbscan_{}.txt".format(str(SAMPLE_NUM)), conf_matrix, delimiter=",", fmt="%d")
        print(conf_matrix)
        
        # 计算每个类的准确率
        class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)
        for idx, accuracy in enumerate(class_accuracy):
            class_name = relations[str(idx+1)]  
            print(f"Accuracy for class {class_name}: {accuracy:.4f}")

        # 或者将每个类的准确率保存到文件
        with open("/home/lxc/CBSeq/data/result/class_accuracy_dbscan_{}.txt".format(str(SAMPLE_NUM)), 'w') as f:
            for idx, accuracy in enumerate(class_accuracy):
                class_name = relations[str(idx+1)]
                f.write(f"Accuracy for class {class_name}: {accuracy:.4f}\n")

        # 打印分类报告
        # 获取唯一的标签和预测
        unique_labels = sorted(set(all_labels))

        # 打印分类报告
        report = classification_report(all_labels, all_preds, target_names=unique_labels, labels=unique_labels)

        print(report)

        # 保存分类报告
        with open("/home/lxc/CBSeq/data/result/classification_report_dbscan_{}.txt".format(str(SAMPLE_NUM)), 'w') as f:
            f.write(report)

    average_loss = total_loss / len(test_loader)
    accuracy = total_correct / total_samples

    print('Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(average_loss, accuracy * 100))


if __name__ == "__main__":
    if not os.path.exists("/home/lxc/CBSeq/data/result/behavior_sequences_embeded_dbscan_{}.json".format(str(SAMPLE_NUM))):
        data_with_labels = preprocess_dbscan.cluster()
        behavior_sequences = preprocess_dbscan.cluster_behavior_sequences(data_with_labels)
        embedded_behavior_sequences = preprocess_dbscan.embedding(behavior_sequences)
        embedded_behavior_sequences = {key: value for key, value in embedded_behavior_sequences.items() if key != '-1'}
    else:
        with open("/home/lxc/CBSeq/data/result/behavior_sequences_embeded_dbscan_{}.json".format(str(SAMPLE_NUM)), 'r') as file:
            data = file.read()
            embedded_behavior_sequences = json.loads(data)
            embedded_behavior_sequences = {key: value for key, value in embedded_behavior_sequences.items() if key != '-1'}

    trial(embedded_behavior_sequences)