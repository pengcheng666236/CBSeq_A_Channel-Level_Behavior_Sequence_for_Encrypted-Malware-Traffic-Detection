import torch
import torch.nn.functional as F

def flatten_clusters(clusters):
    flattened_clusters = []
    for cluster in clusters:
        flattened_cluster = [item for sublist in cluster for item in sublist]
        flattened_clusters.append(flattened_cluster)
    return flattened_clusters


def pad_or_trim(tensor, target_shape):
    current_size = tensor.numel()
    
    target_size = target_shape[0] * target_shape[1]
    print("current size:",current_size,"target size:",target_size)
    if current_size <= target_size:
        # 填充
        pad_size = target_size - current_size
        zeros = torch.zeros(pad_size, dtype=tensor.dtype)
        padded_tensor = torch.cat((tensor, zeros))
    
    print("filled shape:",padded_tensor.numel())
    padded_tensor = tensor[:target_size]
    
    return padded_tensor.view(target_shape)


# 映射函数,把输出的，一个batch的分类序号，转换成类字符串
# 
def map_tensor_to_string(data_array, mapping):
    result = []
    for num in data_array:
        if type(num) is int:
            str_key = str(num)
        else:
            str_key = str(num)
        # print(str_key)
        if str_key in mapping:
            if type(mapping[str_key]) is int:
                result.append(mapping[str_key])
            else:
                result.append(mapping[str_key])
        else:
            result.append('Unknown')  # 处理未找到的键
    return result
