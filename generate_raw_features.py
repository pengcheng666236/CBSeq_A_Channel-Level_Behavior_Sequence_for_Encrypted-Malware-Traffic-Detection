# 从pcap中读取时间间隔内的数据包
from scapy.all import *

def get_f_tuple(packet):
    src_ip = packet.src
    dst_ip = packet.dst
    if "TCP" in packet:
        protocol = 'TCP'
        sport = packet["TCP"].sport
        dport = packet["TCP"].dport
    elif "UDP" in packet:
        protocol = 'UDP'
        sport = packet["UDP"].sport
        dport = packet["UDP"].dport
    else:
        # 其他协议的处理
        sport = None
        dport = None
        protocol = None
    f_tuple = (src_ip,dst_ip,sport,dport,protocol)
    return f_tuple

def calculate_dict_addr(packets):
    dict_addr = []
    dict_addr_reverse = []
    for packet in packets:
        f_tuple = get_f_tuple(packet)
        src_ip,dst_ip,sport,dport,protocal = f_tuple
        reverse_f_tuple = (dst_ip,src_ip,dport,sport,protocal)

        # 全新的
        if f_tuple not in dict_addr and reverse_f_tuple not in dict_addr_reverse and \
            reverse_f_tuple not in dict_addr:
            dict_addr.append(f_tuple)
            dict_addr_reverse.append(reverse_f_tuple)
    return dict_addr

# HyperVision关于流索引的生成：flow_id = {string_2_uint128(sIP), string_2_uint128(dIP), sp, dp};

def process_packets(packets,dict_addr):
    # ip_pair对：cni = (ti , Si ),ti是ip_pair的开始时间，Si是ip_pair对应的flow集合，存储在ip_pairs_details字典
    # Si = {(x1, t1, P1), . . . , (xn, tn, Pn)}，每个流包含一个元组，内容是(通信五元组、流开始时间、对应的流的特征元组)
    # 特征：流持续时间，流数量，总的数据长度，上行数据长度、下载数据长度

    ip_pairs_details = {}
    # 我不知道划分两个flow的时间间隔是多少，所以先用间隔的数据包数量来判断是否是同一个flow，这就导致packet需要一个在pcap文件中的index
    flow_max_interval = 5
    index = 0
    for packet in packets:
        index += 1
        # 尽可能获取当前数据包信息，并更新flow特征和channel特征
        f_tuple = get_f_tuple(packet)
        pkt_len = len(packet)
        pkt_time = packet.time
        # 这里就先默认正向是上传，反正只要统一了，反过来也一样
        pkt_up_length = 0
        pkt_down_length = 0

        if f_tuple in dict_addr:
            pkt_up_length = pkt_len
        else:
            pkt_down_length = pkt_len

        # 更新统计数据
        # 这个数据包属于某个已经存在的channel，就添加到对应的flow的末尾。具体判断有那个flow最后一个数据包到当前数据包的count < 5
        # flow feature:[Stream_duration, pkt_number, total_data_length, up_length, download_length]
        f_tuple_str = "_".join(map(str, f_tuple))
        if f_tuple_str in ip_pairs_details:
            # 获取channel的所有流
            flow_of_channel = ip_pairs_details[f_tuple_str]
            # flow feature：最后一个pkt的index，Stream_duration, pkt_number, total_data_length, up_length, download_length
            added = False
            for flow in flow_of_channel:
                # 属于这个flow,更新对应的统计特征
                # print("cur flow:",flow)
                if index - flow[2] < 5:
                    # flow[0] = f_tuple_str # 这两个本身就是相同的
                    # flow[1] = flow_start_time
                    flow[2] = index # 最后一个pkt的index
                    flow[3] += pkt_time # Stream_duration
                    flow[4] += 1 # pkt_number
                    flow[5] += pkt_len
                    flow[6] += pkt_up_length
                    flow[7] += pkt_down_length
                    added = True
                else:
                    continue
            # 这个channel没有一个flow能匹配的，那就新建一个flow到channel里面
            if not added:
                new_flow = [f_tuple_str,pkt_time,     index,pkt_time,1,pkt_len,pkt_up_length,pkt_down_length]
                # print("new flow:",new_flow)
                flow_of_channel.append(new_flow)
        else:
            # 没有channel包含这个pkt的五元组，是一个全新的五元组，则新建一个channel，里面包含这个新的flow
            new_flow = [f_tuple_str,pkt_time,     index,pkt_time,1,pkt_len,pkt_up_length,pkt_down_length]
            # print("new channel:",new_flow)
            ip_pairs_details[f_tuple_str] = [new_flow]
    return ip_pairs_details



def write_results_to_file(ip_pairs_details, output_file):
    with open(output_file, 'w') as f:
        f.write("f_tuple,flow_start_time,Stream_duration,pkt_number,total_data_length,up_length,down_length\n")
        # print(list(ip_pairs_details.values())[0])
        # [['205.188.12.91_10.8.8.178_443_48911_TCP', Decimal('1433356821.839550'), 38, Decimal('25800423284.912380'), 18, 3478, 0, 3478], 
        #  ['205.188.12.91_10.8.8.178_443_48911_TCP', Decimal('1433356875.049500'), 53, Decimal('8600141276.966972'), 6, 1218, 0, 1218]]
        for channel in ip_pairs_details.values():
            for flow in channel:
                del(flow[2])
                flow_data = [str(item) for item in flow]
                f.write(','.join(flow_data) + '\n')



if __name__ == "__main__":
    # pcap_file = "/home/lxc/Datasets/ISCX-VPN-NonVPN/ISCX-VPN/vpn_aim_chat1a.pcap"
    # output_file = "/home/lxc/CBSeq/data/vpn/"+pcap_file.split("/")[-1].replace(".pcap",".csv")
    # packets = rdpcap(pcap_file)
    # addr_dict = calculate_dict_addr(packets)
    # ip_pair_details = process_packets(packets,addr_dict)
    # write_results_to_file(ip_pair_details, output_file)

    data_list = ["/home/lxc/Datasets/ISCX-VPN-NonVPN/ISCX-NonVPN",
                 "/home/lxc/Datasets/ISCX-VPN-NonVPN/ISCX-VPN"]
    dst_list = ["/home/lxc/CBSeq/data/nonvpn/",
                "/home/lxc/CBSeq/data/vpn/"]
    idx = 0
    for root, dirs, files in os.walk(data_list[idx]):
        for file in files:
            print("cur file:",file)
            pcap_file = os.path.join(root, file)
            output_file = dst_list[idx] + pcap_file.split("/")[-1].replace(".pcapng",".txt").replace(".pcap",".txt")
            if os.path.exists(output_file):
                print("processed:",output_file)
                continue
            packets = rdpcap(pcap_file)
            addr_dict = calculate_dict_addr(packets)
            ip_pair_details = process_packets(packets,addr_dict)
            write_results_to_file(ip_pair_details, output_file)