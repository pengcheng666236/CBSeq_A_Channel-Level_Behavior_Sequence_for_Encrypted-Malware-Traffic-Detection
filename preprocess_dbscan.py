from collections import Counter, defaultdict
import json
import os
from statistics import mode
from sklearn.cluster import DBSCAN
import numpy as np
from gensim.models import Word2Vec
import tools
SAMPLE_NUM = 10000
max_sequence_length = 16

def cluster():
    data = []
    f_tuples = []
    flow_start_time = []

    files = []
    nonvpn_raw_files = os.listdir("/home/lxc/CBSeq/data/nonvpn")
    vpn_raw_files = os.listdir("/home/lxc/CBSeq/data/vpn")
    for file in nonvpn_raw_files:
        files.append(os.path.join("/home/lxc/CBSeq/data/nonvpn", file))
    for file in vpn_raw_files:
        files.append(os.path.join("/home/lxc/CBSeq/data/vpn", file))

    relations = {}
    relations_reverse = {}
    sample_nums = {}
    dbscan_results = {}
    file_num = 0
    for file in files:
        label_file = file.split("/")[-1].split(".")[0]
        file_num += 1

        relations[file_num] = label_file
        relations_reverse[label_file] = file_num
        print("cur file:",file,file_num)
        with open(file, "r") as file:
            file.readline()
            # 限制样本数
            sample_num=0
            for line in file:
                # 将每行数据拆分成各个属性
                row = line.strip().split(',')

                f_tuples.append(row[0])
                flow_start_time.append(row[1])  # 保存 flow_start_time 信息

                flow_features = row[2:]
                flow_features.append(label_file)
                # print(flow_features)

                data.append(flow_features)
                sample_num += 1
                if sample_num > SAMPLE_NUM:
                    break
            sample_nums[label_file] = sample_num
    
    with open('/home/lxc/CBSeq/data/result/sample_nums_dbscan_{}.json'.format(str(SAMPLE_NUM)), 'w') as json_file:
        json.dump(sample_nums, json_file)
    
    data = np.array(data)
    # 预聚类只选择四个sequence特征
    # print("sample:",data[0])
    pre_cluster_data = [sample[:-1] for sample in data]
    # print("pre_cluster_data:",pre_cluster_data[0])
    pre_cluster_data = [[float(item) for item in sample] for sample in pre_cluster_data]

    dbscan = DBSCAN(eps=1).fit(pre_cluster_data)
    labels = dbscan.labels_

    # 处理噪声点
    
    # 获取噪声点的索引
    noise_indices = np.where(dbscan == -1)[0]
    print("len Noise:",len(noise_indices)) # 83600
    # 为噪声点分配新的聚类标签
    for idx, noise_idx in enumerate(noise_indices):
        labels[noise_idx] = np.max(dbscan) + idx + 1

    # 计算聚类的个数（已处理噪声点）
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    print(f'聚类的个数: {n_clusters}')


    # 统计每个聚类中的点数量
    label_counts = Counter(labels)
    for label, count in label_counts.items():
        cluster_name = f'Cluster {label}' if label != -1 else 'Noise'
        print(f'{cluster_name} 包含的点数量: {count}')
        dbscan_results[cluster_name] = count
        


    # 保存映射关系
    if not os.path.exists('/home/lxc/CBSeq/data/result/relations_dbscan.json'):
        try:
            with open('/home/lxc/CBSeq/data/result/relations_dbscan.json', 'r') as json_file:
                existing_data = json.load(json_file)
        except FileNotFoundError:
            existing_data = {}
        existing_data.update(relations)
        with open('/home/lxc/CBSeq/data/result/relations_dbscan.json', 'w') as json_file:
            json.dump(existing_data, json_file)
    
    if not os.path.exists('/home/lxc/CBSeq/data/result/relations_reverse_dbscan.json'):
        try:
            with open('/home/lxc/CBSeq/data/result/relations_reverse_dbscan.json', 'r') as json_file:
                existing_data = json.load(json_file)
        except FileNotFoundError:
            existing_data = {}
        existing_data.update(relations_reverse)
        with open('/home/lxc/CBSeq/data/result/relations_reverse_dbscan.json', 'w') as json_file:
            json.dump(existing_data, json_file)

    
    with open('/home/lxc/CBSeq/data/result/dbscan_result{}.json'.format(str(SAMPLE_NUM)), 'w') as json_file:
        json.dump(dbscan_results, json_file)

    data_with_labels = np.column_stack((np.array(f_tuples),np.array(flow_start_time), data, labels))
    """
        data_with_labels:二维数组,每一行是一个flow.
            每个flow包含:f_tuple,flow_start_time,
            Stream_duration,pkt_number,total_data_length,up_length,download_length,
            真实的label，预聚类的label

        这样就可以做到:根据预聚类标签，只对预聚类内部的元素进行特征提取，构建behavior sequence.
        也就是用预聚类label作为特征提取的单元，真实label用于判别准确率
        
    """
    return data_with_labels.tolist()
    

def slice_cluster_with_time(data_with_labels, time_window):
    # 创建一个字典，用于存储按时间窗口切分后的数据
    sliced_clusters = {}

    # 预聚类
    for row in data_with_labels:
        # 获取样本数据
        data = row[:-1]  # 去除最后一个元素（聚类标签）
        # 获取时间戳（假设时间戳在数据的第二个位置）
        timestamp = row[1]

        # 计算时间窗口索引
        window_index = int(timestamp / time_window)

        if window_index not in sliced_clusters:
            sliced_clusters[window_index] = []

        sliced_clusters[window_index].append(data)

    return sliced_clusters

# 给定一个cluster，把里面所有的channel提取出特征序列
def extract_sequences(cluster_data):
    pn_cluster = []
    iat_cluster = []
    sp_cluster = []
    dp_cluster = []
    # channel就是一组:五元组，且数据包间隔小于5的flow的集合
    # 限制cluster的sequence长度为16
    for channel_data in cluster_data[:max_sequence_length]:
        """
        [
            "00:ff:c1:e4:8c:db_00:ff:c2:e4:8c:db_53075_11666_UDP",
            "1432312400.722088",
            "3709689184923.34",
            "2590.0",
            "323750.0",
            "323750.0",
            "0.0",
            "vpn_aim_chat_1",
            "-1"
        ],
        """
        pn_channel = []
        iat_channel = []
        sp_channel = []
        dp_channel = []
        pre_flow_time = -1
        for flow in channel_data:
            # print(flow)
            # 假设通道数据的格式为：[f_tuple,flow_start_time, Stream_duration, pkt_number, total_data_length, up_length, download_length]
            f_tuple = flow[0]
            flow_start_time = flow[1]
            stream_duration = flow[2]
            pkt_number = flow[3]
            total_data_length = flow[4]
            up_length = flow[5]
            download_length = flow[6]

            # 提取对应的特征序列,分别表示channel内：每个flow的包数量，每个流的时间间隔，每个flow的源端口，每个flow的目的端口
            pn = pkt_number
            if pre_flow_time == -1:
                iat = 0
                pre_flow_time = flow_start_time
            else:
                iat = str(float(flow_start_time) - float(pre_flow_time))
                pre_flow_time = flow_start_time
            
            sp = f_tuple.split("_")[2]
            dp = f_tuple.split("_")[3]
            # print(pn,iat,sp,dp)
            # 添加到当前channel的列表中
            pn_channel.append(str(pn))
            iat_channel.append(str(iat))
            sp_channel.append(str(sp))
            dp_channel.append(str(dp))

        pn_cluster.append(pn_channel)
        iat_cluster.append(iat_channel)
        sp_cluster.append(sp_channel)
        dp_cluster.append(dp_channel)

    return pn_cluster, iat_cluster, sp_cluster, dp_cluster
    # pn_cluster:[pn_channel1的原始特征序列,pn_channel2的原始特征序列,...]
    # pn_channel1:[pn_flow1的特征值,pn_flow2的特征值,...]


# 把属于同一个cluster的channel的四个特征序列拼接起来，构成完整的特征表示
def cluster_behavior_sequences(data_with_labels):
    # 预聚类
    clusters = {}

    for row in data_with_labels:
        f_tuple = str(row[0])
        cluster_label = str(row[-1])  # 最后一个元素是dbscan的聚类标签
        # 没有这个聚类条目，则创建新的聚类条目
        if cluster_label not in clusters.keys():
            clusters[cluster_label] = [[]]  # 初始化聚类条目对应的，一个空的信道列表
            clusters[cluster_label][0].append(row)  # 向信道列表的第一个信道添加新的流
        else:
            found_channel = False
            for channel in clusters[cluster_label]:
                # 这个信道的，第一个flow的，第一个特征元素就是索引/f_tuple
                # print("cur pair:",f_tuple,channel[0][0])
                if channel and channel[0][0] == f_tuple:
                    channel.append(row)
                    found_channel = True
                    break
            # 没有这个f_tuple的channel，就新建一个channel
            if not found_channel:
                clusters[cluster_label].append([row])

    # 将字典样本写入 JSON 文件
    with open('/home/lxc/CBSeq/data/result/clusters_sample_dbscan_{}.json'.format(str(SAMPLE_NUM)), 'w') as json_file:
        sample = {}
        for cluster_label,value in clusters.items():
            sample[cluster_label] = value[:10]
        json.dump(sample, json_file)
    """
    "-1": [
        [
            "00:ff:c1:e4:8c:db_00:ff:c2:e4:8c:db_53075_11666_UDP",
            "1432312400.722088",
            "3709689184923.34",
            "2590.0",
            "323750.0",
            "323750.0",
            "0.0",
            "vpn_aim_chat_1",
            "-1"
        ],
    """
    behavior_sequences = {}
    """
        这就是论文预聚类的操作：对聚类内的所有channel的特征进行合并
        论文说通过控制eps，可以控制聚类内的点一定是同一类的
        还说可以通过划分时间窗口，来确保每个子聚类的点议定书同一个时间窗口的

        标签是按flow给的，channel的标签等于其中任何一个flow的标签，这个是绝对一致的
        方案1，选择cluster的标签众数，所有channel换标签
        方案2，选择cluster的标签众数，不是这个标签的channel从cluster中剔除
        我选方案2
    """

    # 对聚类内部的元素做筛选，确保聚类内部的channel都是同一个标签的
    # 鉴于论文的behavior sequence都是同一个标签的样本的，我需要对提取behavior之前的cluster数据做一次筛选

    cluster_acc = {} # 额外计算一下每个聚类的聚类准确性，即聚类有多少样本不属于这个类的众数
    for cluster_label, cluster_data in clusters.items():
        cur_cluster_tp = 0
        # print("cur cluster:",cluster_label)

        # 对每个聚类内的样本按照开始时间进行排序
        # 先对channel内的flow进行排序，然后根据channel最早的，即第一个flow的时间，对channel排序
        for channel in cluster_data:
            # print("cur channel:",channel)
            channel.sort(key=lambda x: x[1])
        cluster_data.sort(key=lambda x: x[0][1])  # channel开始时间是：第一个flow的第二个索引的值


        # 提取cluster最多的类别标签
        filtered_cluster_data = []
        cluster_most_freq_labels = []
        for channel in cluster_data:
            # 所有channel内部的label相同，取第一个就够了
            cluster_most_freq_labels.append(channel[0][-2])
        cluster_most_freq_label = mode(cluster_most_freq_labels)
    

        for channel in cluster_data:
            if channel[0][-2] == cluster_most_freq_label:
                cur_cluster_tp += 1
                filtered_cluster_data.append(channel)
        
        cluster_acc[cluster_label] = [(cur_cluster_tp / len(cluster_data)),cur_cluster_tp]
        

        # 把cluster的channel特征连接起来，组成behavior
        # 一个cluster内的所有flow都是相同或相反的四元组+相同的协议
        pn_cluster, iat_cluster, sp_cluster, dp_cluster = extract_sequences(filtered_cluster_data)

        # 打印cluster前10个channel的pn特征序列，根据包含的flow数量不同而长度不同
        # 打印证明，flatten_clusters会导致不足三层嵌套的list被多切分一次，导致小数被拆分成字符，因此不用
        # print("pn_cluster[:10] before:",pn_cluster[:10])
        # flattened_pn_cluster = tools.flatten_clusters(pn_cluster)
        # print("pn_cluster[:10] after:",flattened_pn_cluster[:10])

        # 连接4个cluster的特征序列，构成聚类内部的完整行为序列，聚类标签不动，聚类类别等于cluster_most_freq_label
        behavior_sequences[cluster_label] = {
            'pn_cluster': pn_cluster,
            'iat_cluster': iat_cluster,
            'sp_cluster': sp_cluster,
            'dp_cluster': dp_cluster,
            'file_label': cluster_most_freq_label
        }
    # 将字典写入JSON文件
    with open('/home/lxc/CBSeq/data/result/behavior_sequences_dbscan_{}.json'.format(str(SAMPLE_NUM)), 'w') as json_file:
        json.dump(behavior_sequences, json_file)
    
    with open('/home/lxc/CBSeq/data/result/dbscan_acc_{}.json'.format(str(SAMPLE_NUM)), 'w') as json_file:
        json.dump(cluster_acc, json_file)

    return behavior_sequences


"""
聚类的目的是，更好的表示聚类内部的特征
但是聚类的标准是，相同五元组的，相同时间窗口(未实现)的一组channel,所以单独训练的话，sp和dp的模型没有意义
策略：把每个类的sequence合并，集体训练，然后对每个类分别使用模型嵌入
也就是在保持局类结构的情况下，进行词嵌入表示
每个聚类都对应一个be
"""

# 这个函数不也看出来了吗，你的聚类的不会对特征的提取产生影响
def embedding(behavior_sequences):
    pn_all = []
    iat_all = []
    sp_all = []
    dp_all = []
    # 汇总训练
    for cluster_label, cluster_data in behavior_sequences.items():
        pn_cluster = cluster_data['pn_cluster']
        iat_cluster = cluster_data['iat_cluster']
        sp_cluster = cluster_data['sp_cluster']
        dp_cluster = cluster_data['dp_cluster']
        pn_all.append(pn_cluster)
        iat_all.append(iat_cluster)
        sp_all.append(sp_cluster)
        dp_all.append(dp_cluster)
    # 这里原来是三层，需要去掉一层，换成一行一个句子的形式，每个取值是一个词
    # print("pn_all before:",pn_all[:10])
    pn_all = tools.flatten_clusters(pn_all)
    # print("pn_all after:",pn_all[:10])
    iat_all = tools.flatten_clusters(iat_all)
    sp_all = tools.flatten_clusters(sp_all)
    dp_all = tools.flatten_clusters(dp_all)

    pn_model = Word2Vec(sentences=pn_all, vector_size=100, window=5, min_count=1, workers=4, sg=0)
    iat_model = Word2Vec(sentences=iat_all, vector_size=100, window=5, min_count=1, workers=4, sg=0)
    sp_model = Word2Vec(sentences=sp_all, vector_size=100, window=5, min_count=1, workers=4, sg=0)
    dp_model = Word2Vec(sentences=dp_all, vector_size=100, window=5, min_count=1, workers=4, sg=0)
    # 分散嵌入
    for cluster_label, cluster_data in behavior_sequences.items():
        pn_cluster = cluster_data['pn_cluster']
        iat_cluster = cluster_data['iat_cluster']
        sp_cluster = cluster_data['sp_cluster']
        dp_cluster = cluster_data['dp_cluster']
        embedded_pn_cluster = [pn_model.wv[word].tolist() for channel in pn_cluster for word in channel if word in pn_model.wv]
        embedded_iat_cluster = [iat_model.wv[word].tolist() for channel in iat_cluster for word in channel if word in iat_model.wv]
        embedded_sp_cluster = [sp_model.wv[word].tolist() for channel in sp_cluster for word in channel if word in sp_model.wv]
        embedded_dp_cluster = [dp_model.wv[word].tolist() for channel in dp_cluster for word in channel if word in dp_model.wv]
        
        """
            不是把  cluster_data['embedded_pn_cluster'] = embedded_pn_cluster 对齐到 NUM_HEADS * EMBED_SIZE
            因为在sample的时候，是一个个取cluster的behavior sequence的embedding值
        """
        
        cluster_data['embedded_pn_cluster'] = embedded_pn_cluster
        cluster_data['embedded_iat_cluster'] = embedded_iat_cluster
        cluster_data['embedded_sp_cluster'] = embedded_sp_cluster
        cluster_data['embedded_dp_cluster'] = embedded_dp_cluster
        # print(len(embedded_pn_cluster),",",len(embedded_iat_cluster),",",len(embedded_iat_cluster),",",len(embedded_iat_cluster))

    
    # 将字典写入 JSON 文件,每个值被映射到一个很长的向量，也就是词向量，同一个值是相同的
    # 在那个空间上，相似的端口在一起，只是我看不见
    with open('/home/lxc/CBSeq/data/result/behavior_sequences_embeded_dbscan_{}.json'.format(str(SAMPLE_NUM)), 'w') as json_file:
        sample_behavior_sequences = dict(list(behavior_sequences.items()))
        json.dump(sample_behavior_sequences, json_file)
    
    return behavior_sequences
